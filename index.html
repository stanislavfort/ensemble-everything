<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Ensemble everything everywhere: Multi-scale aggregation for adversarial robustness">
  <meta property="og:title" content="Ensemble everything everywhere: Multi-scale aggregation for adversarial robustness"/>
  <meta property="og:description" content="Ensemble everything everywhere: Multi-scale aggregation for adversarial robustness"/>
  <meta property="og:url" content="https://stanislavfort.com/ensemble-everything/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/headline.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Ensemble everything everywhere: Multi-scale aggregation for adversarial robustness">
  <meta name="twitter:description" content="Ensemble everything everywhere: Multi-scale aggregation for adversarial robustness">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/image/headline.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="adversarial attacks, robustness, generator">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Ensemble Everything Everywhere</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Ensemble everything everywhere:<br>Multi-scale aggregation for adversarial robustness</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <b></b><a href="https://scholar.google.com/citations?user=eu2Kzn0AAAAJ" target="_blank">Stanislav Fort</a></b><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.co.uk/citations?user=QYn8RbgAAAAJ" target="_blank">Balaji Lakshminarayanan</a></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Google DeepMind<br></span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Main contributor and project lead</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2408.05446" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/stanislavfort/ensemble-everything-everywhere" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2408.05446" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

  <section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <img src="static/images/headline.png" alt="The headline figure showing the key aspects of the multi-resolution self-ensemble" class="center-image"/>
        </div>
        <h2 class="subtitle has-text-centered">
        We use a multi-resolution decomposition (a) of an input image and a partial decorrelation of predictions of intermediate layers (b) to build a classifier (c) that has, by default, adversarial robustness comparable or exceeding state-of-the-art (f), even without any adversarial training. Optimizing inputs against it leads to interpretable changes (d) and images generated from scratch (e).
      </h2>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Adversarial examples pose a significant challenge to the robustness, reliability and alignment of deep neural networks. We propose a novel, easy-to-use approach to achieving high-quality representations that lead to adversarial robustness through the use of multi-resolution input representations and dynamic self-ensembling of intermediate layer predictions. We demonstrate that intermediate layer predictions exhibit inherent robustness to adversarial attacks crafted to fool the full classifier, and propose a robust aggregation mechanism based on Vickrey auction that we call \textit{CrossMax} to dynamically ensemble them. By combining multi-resolution inputs and robust ensembling, we achieve significant adversarial robustness on CIFAR-10 and CIFAR-100 datasets without any adversarial training or extra data, reaching an adversarial accuracy of ≈72% (CIFAR-10) and ≈48% (CIFAR-100) on the RobustBench AutoAttack suite (L∞=8/255) with a finetuned ImageNet-pretrained ResNet152. This represents a result comparable with the top three models on CIFAR-10 and a +5 % gain compared to the best current dedicated approach on CIFAR-100. Adding simple adversarial training on top, we get ≈78% on CIFAR-10 and ≈51% on CIFAR-100, improving SOTA by 5 % and 9 % respectively and seeing greater gains on the harder dataset. We validate our approach through extensive experiments and provide insights into the interplay between adversarial robustness, and the hierarchical nature of deep representations. We show that simple gradient-based attacks against our model lead to human-interpretable images of the target classes as well as interpretable image changes. As a byproduct, using our multi-resolution prior, we turn pre-trained classifiers and CLIP models into controllable image generators and develop successful transferable attacks on large vision language models.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

  <section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
      <h2 class="title is-3">Multi-resolution input to mimic the human eye</h2>
      <div class="level-set has-text-justified">
        <p>
          Humans do not take a single static picture with their eyes and classify it. Instead, many noisy, jittered frames are effectively captured at different resolutions and a classification is performed on all of them at once. To mimic this, we train a neural network to accept a channel-wise <b>multi-resolution</b> stack of images <it>at once</it>. This automatically leads to significant adversarial robustness of the learned model, <it>provided that very low learning rate is used.</it> 
          </p>
     </div>
      <div class="content">
     <img src="static/images/multi-resolution.png" alt="A multi-resolution architecture taking a channel-wise, multi-resolution stack of images as an input" class="blend-img-background center-image"/>
      </div>
   </div>
 </div>
</div>
</section>

  
  <section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
      <h2 class="title is-3">Standard adversarial attacks only fool the final layer</h2>
      <div class="level-set has-text-justified">
        <p>
          We experimentally demonstrate that standard adversarial attacks on a classifier only fool the very final layers of the network. In other words, a <it>dog</it> attacked to look like a <it>car</it> still has predominantly <it>dog</it>-like early and intermediate layer representations such as oriented edges, textures, and even higher level features. 
          </p>
     </div>
      <div class="content">
     <img src="static/images/only-last-layer-attacked.png" alt="An adversarial attack on a classifier only confuses the representations at the very last layers of the neural network" class="blend-img-background center-image"/>
      </div>
    <div class="level-set has-text-justified">
        <p>
        Attacks that perturb an image to confuse a particular layer are mostly effective only for the layer itself and the layers surrounding it. Representations from layers before and even after partially recover and see the original ground truth class instead of the attack target class.
        </p>
     </div>
    <div class="content">
     <img src="static/images/layer-decorrelation.png" alt="An adversarial attack oconfusing a particular layer does not confuse the layers before and even after" class="blend-img-background center-image"/>
      </div>
   </div>
 </div>
</div>
</section>

<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
      <h2 class="title is-3">Ensembling intermediate predictions via <it>CrossMax</it> => multi-resolution self-ensemble</h2>
      <div class="level-set has-text-justified">
        <p>
          We use the partial layer susceptibility decorrelation to construct a <it>self-ensemble</it> by ensembling the predictions of the intermediate layers (extracted with trained linear probes). To avoid non-robust aggregation that could be dominated by a single layer or class, we propose a new ensembling algorithm called <it>CrossMax</it>.
          </p>
     </div>
      <div class="content">
     <img src="static/images/crossmax.png" alt="A new ensembling algorithm called CrossMax" class="blend-img-background center-image"/>
      </div>
    <div class="level-set has-text-justified">
        <p>
        Combining the multi-resolution input with intermediate layer self-ensembling via <it>CrossMax</it> leads to SOTA or above SOTA white-box adversarial robustness.
        </p>
     </div>
    <div class="content">
     <img src="static/images/self-ensemble.png" alt="A multi-resolution self-ensemble" class="blend-img-background center-image"/>
      </div>
   </div>
 </div>
</div>
</section>

<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
      <h2 class="title is-3">Adversarial robustness on RobustBench</h2>
      <div class="level-set has-text-justified">
        <p>
          Our multi-resolution self-ensemble <it>without any adversarial training at all</it> reaches similar results to SOTA on the <a href="https://robustbench.github.io/">RobustBench</a> white box AutoAttack attack suite on CIFAR-10 and improves upon SOTA by +5% on CIFAR-100. With very light adversarial training (2x compute overhead), we surpass SOTA on CIFAR-10 by +5% and on CIFAR-100 by +9%. This is despite competing methods using 100x—1000x more compute for adversarial training. 
          </p>
     </div>
      <div class="content">
     <img src="static/images/results.png" alt="Results for adversarial robustness on RobustBench" class="blend-img-background center-image"/>
      </div>
   </div>
 </div>
</div>
</section>

<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
      <h2 class="title is-3">Gradient ascent on pixels towards class => interpretable image</h2>
      <div class="level-set has-text-justified">
        <p>
          Our model is so robust and aligned with human-like visual features that directly optimizing the input image to increase the probability of the target class generates interpretable images of the semantic content of the class. Normally, this would lead to uninterpretable noise. We therefore effectively unify classification and generation. Examples of 4 such generated "attacks" starting from grey pixels and maximizing the probability of a target class for our CIFAR-100 model:
          </p>
     </div>
      <div class="content">
     <img src="static/images/cifar100-generation-subset.png" alt="Results for adversarial robustness on RobustBench" class="blend-img-background center-image"/>
      </div>
   </div>
 </div>
</div>
</section>

  <section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
      <h2 class="title is-3">If an attack succeeds, we see why</h2>
      <div class="level-set has-text-justified">
        <p>
          Due to the robustness of our model, successful attacks end up looking very interpretable and we see why the class decision has been changed.
          </p>
     </div>
      <div class="content">
     <img src="static/images/interpretable-attacks.png" alt="When an adversarial attack succeeds, we see why the image has been misclassified" class="blend-img-background center-image"/>
      </div>
   </div>
 </div>
</div>
</section>


  <section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
      <h2 class="title is-3">Bonus 1: Pretrained CLIP is now a generator with no additional training</h2>
      <div class="level-set has-text-justified">
        <p>
          Using the multi-resolution prior and flipping it around, we show that if we express an adversarial perturbation as a sum of perturbations at different resolutions and optimize them at once, the resulting images will be very interpretable. Doing this with a CLIP model, we can steer the "attack" towards a particular text embedding, effectively creating an image generator for free, with no diffusion, GANs, or training involved at any stage.
          </p>
     </div>
      <div class="content">
     <img src="static/images/clip-generation.png" alt="Using the multi-resolution prior to turn a pretrained CLIP into an image generator" class="blend-img-background center-image"/>
      </div>
      <div class="level-set has-text-justified">
        <p>
          Starting from an image and "attacking" it towards a label, the changes we get are extremely interpretable and effectively boil down to photo manipulation.
        </p>
     </div>
    <div class="content">
     <img src="static/images/newton-einstein.png" alt="A multi-resolution attack on CLIP turning Isaac Newton to Albert Einstein in an interpretable way" class="blend-img-background center-image"/>
      </div>
   </div>
 </div>
</div>
</section>

  <section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
      <h2 class="title is-3">Bonus 2: The first transferrable image attacks on GPT-4, Claude 3, and Bing AI</h2>
      <div class="level-set has-text-justified">
        <p>
          Using the same multi-resolution prior, we constructed the first <b>transferrable</b> image attacks on state-of-the-art closed-source vision-LLMs such as GPT-4, Claude 3 and Bing AI.
          </p>
     </div>
      <div class="content">
     <img src="static/images/vLLM-attacks.png" alt="Transferrable image attacks on GPT-4o" class="blend-img-background center-image"/>
      </div>
   </div>
 </div>
</div>
</section>

  <!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/gpt4o-rickroll-v2.mov"
        type="video/mov">
      </video>
      <h2 class="subtitle has-text-centered">
       Rickrolling GPT-4o into seeing the video "Never Gonna Give You Up" by Rick Astley in a specially modified photo of Stephen Hawking.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{fort2024ensembleeverywheremultiscaleaggregation,
      title={Ensemble everything everywhere: Multi-scale aggregation for adversarial robustness}, 
      author={Stanislav Fort and Balaji Lakshminarayanan},
      year={2024},
      eprint={2408.05446},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2408.05446}, 
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- Default Statcounter code for Ensemble Everything Website
https://stanislavfort.com/ensemble-everything/ -->
<script type="text/javascript">
var sc_project=13043140; 
var sc_invisible=1; 
var sc_security="23cfbaa7"; 
</script>
<script type="text/javascript"
src="https://www.statcounter.com/counter/counter.js" async></script>
<noscript><div class="statcounter"><a title="Web Analytics"
href="https://statcounter.com/" target="_blank"><img class="statcounter"
src="https://c.statcounter.com/13043140/0/23cfbaa7/1/" alt="Web Analytics"
referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
<!-- End of Statcounter Code -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
